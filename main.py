import os
import json
import pickle
import re
import random
from datetime import datetime
from typing import List, Dict, Tuple, Optional
import logging

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import (
    AutoTokenizer, AutoModel, AutoConfig,
    get_linear_schedule_with_warmup,
    TrainingArguments, Trainer
)

import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
import faiss
import gradio as gr
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity

# ê°€ì†í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import xformers
    import xformers.ops
    XFORMERS_AVAILABLE = True
except ImportError:
    XFORMERS_AVAILABLE = False

try:
    import triton
    TRITON_AVAILABLE = True
except ImportError:
    TRITON_AVAILABLE = False

try:
    from flash_attn import flash_attn_func
    FLASH_ATTN_AVAILABLE = True
except ImportError:
    FLASH_ATTN_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class StyleEmbeddingModel(nn.Module):
    """ë¬¸ì²´ ì„ë² ë”©ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ëª¨ë¸"""
    
    def __init__(self, base_model_name: str = "klue/bert-base", 
                 embedding_dim: int = 384, num_style_classes: int = 8,
                 use_flash_attn: bool = False):
        super().__init__()
        
        self.config = AutoConfig.from_pretrained(base_model_name)
        self.base_model = AutoModel.from_pretrained(base_model_name)
        
        # í”Œë˜ì‹œ ì–´í…ì…˜ ì„¤ì •
        if use_flash_attn and FLASH_ATTN_AVAILABLE:
            self.config.use_flash_attention_2 = True
            logger.info("Flash Attention 2 enabled")
        
        # ì„ë² ë”© í”„ë¡œì ì…˜ ë ˆì´ì–´
        self.style_projector = nn.Sequential(
            nn.Linear(self.config.hidden_size, embedding_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(embedding_dim * 2, embedding_dim),
            nn.LayerNorm(embedding_dim)
        )
        
        # ë¬¸ì²´ ë¶„ë¥˜ í—¤ë“œ
        self.style_classifier = nn.Sequential(
            nn.Linear(embedding_dim, embedding_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(embedding_dim // 2, num_style_classes)
        )
        
        self.embedding_dim = embedding_dim
        
    def forward(self, input_ids, attention_mask, labels=None):
        # Base model forward
        outputs = self.base_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True
        )
        
        # CLS í† í° ì‚¬ìš©
        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token
        
        # ìŠ¤íƒ€ì¼ ì„ë² ë”© ìƒì„±
        style_embeddings = self.style_projector(pooled_output)
        
        # ë¬¸ì²´ ë¶„ë¥˜
        style_logits = self.style_classifier(style_embeddings)
        
        loss = None
        if labels is not None:
            loss_fn = nn.CrossEntropyLoss()
            loss = loss_fn(style_logits, labels)
        
        return {
            'loss': loss,
            'style_embeddings': style_embeddings,
            'style_logits': style_logits,
            'pooled_output': pooled_output
        }

class StyleDataset(Dataset):
    """ë¬¸ì²´ í•™ìŠµìš© ë°ì´í„°ì…‹"""
    
    def __init__(self, texts: List[str], labels: List[int], 
                 tokenizer, max_length: int = 512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

class StyleDataProcessor:
    """ë¬¸ì²´ ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.style_patterns = {
            'formal': ['ìŠµë‹ˆë‹¤', 'ì…ë‹ˆë‹¤', 'í•˜ì˜€ìŠµë‹ˆë‹¤', 'ë˜ì—ˆìŠµë‹ˆë‹¤', 'ì˜€ìŠµë‹ˆë‹¤'],
            'informal': ['ì•¼', 'ì–´', 'ì§€', 'ì–ì•„', 'ê±°ì•¼', 'í•´'],
            'literary': ['ì²˜ëŸ¼', 'ë§ˆì¹˜', 'ë“¯ì´', 'ê²ƒë§Œ ê°™ì•˜ë‹¤', 'ìŠ¤ë©°ë“¤ì–´', 'í˜ëŸ¬'],
            'dialogue': ['"', "'", 'ë¼ê³ ', 'í–ˆë‹¤', 'ë§í–ˆë‹¤', 'ë¬¼ì—ˆë‹¤'],
            'narrative': ['ê·¸ëŠ”', 'ê·¸ë…€ëŠ”', 'ì´ë•Œ', 'ê·¸ ìˆœê°„', 'í•œí¸', 'ê·¸ëŸ¬ë‚˜'],
            'poetic': ['ë‹¬ë¹›', 'ë°”ëŒ', 'ê½ƒì', 'ë³„', 'êµ¬ë¦„', 'ë…¸ì„'],
            'technical': ['ì‹œìŠ¤í…œ', 'ë°ì´í„°', 'ë¶„ì„', 'ê²°ê³¼', 'ë°©ë²•', 'ê³¼ì •'],
            'emotional': ['ê°€ìŠ´', 'ë§ˆìŒ', 'ëˆˆë¬¼', 'ê¸°ì¨', 'ìŠ¬í””', 'ë¶„ë…¸']
        }
        
        self.label_map = {
            'formal': 0, 'informal': 1, 'literary': 2, 'dialogue': 3,
            'narrative': 4, 'poetic': 5, 'technical': 6, 'emotional': 7
        }
        
    def extract_sentences(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ì¥ ì¶”ì¶œ"""
        # ë¬¸ì¥ ë¶„ë¦¬
        sentences = re.split(r'[.!?]\s*', text)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        return sentences
    
    def classify_style(self, sentence: str) -> int:
        """ë¬¸ì¥ì˜ ë¬¸ì²´ ë¶„ë¥˜"""
        sentence = sentence.lower()
        style_scores = {}
        
        for style, patterns in self.style_patterns.items():
            score = sum(1 for pattern in patterns if pattern in sentence)
            style_scores[style] = score
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ìŠ¤íƒ€ì¼ ì„ íƒ
        if max(style_scores.values()) == 0:
            return self.label_map['narrative']  # ê¸°ë³¸ê°’
        
        best_style = max(style_scores, key=style_scores.get)
        return self.label_map[best_style]
    
    def create_dataset(self, book_text: str, min_sentence_length: int = 20) -> Tuple[List[str], List[int]]:
        """ì±… í…ìŠ¤íŠ¸ë¡œë¶€í„° ë¬¸ì²´ ë°ì´í„°ì…‹ ìƒì„±"""
        sentences = self.extract_sentences(book_text)
        
        # ê¸¸ì´ í•„í„°ë§
        sentences = [s for s in sentences if len(s) >= min_sentence_length]
        
        # ë¬¸ì²´ ë¶„ë¥˜
        texts = []
        labels = []
        
        for sentence in sentences:
            label = self.classify_style(sentence)
            texts.append(sentence)
            labels.append(label)
        
        # ë°ì´í„° ì¦ê°•: ë¬¸ì¥ ì¡°í•©
        augmented_texts, augmented_labels = self.augment_data(texts, labels)
        texts.extend(augmented_texts)
        labels.extend(augmented_labels)
        
        return texts, labels
    
    def augment_data(self, texts: List[str], labels: List[int], 
                    augment_ratio: float = 0.3) -> Tuple[List[str], List[int]]:
        """ë°ì´í„° ì¦ê°•"""
        augmented_texts = []
        augmented_labels = []
        
        num_augment = int(len(texts) * augment_ratio)
        
        for _ in range(num_augment):
            # ê°™ì€ ìŠ¤íƒ€ì¼ì˜ ë¬¸ì¥ë“¤ ì¡°í•©
            idx1, idx2 = random.sample(range(len(texts)), 2)
            if labels[idx1] == labels[idx2]:
                combined_text = f"{texts[idx1]} {texts[idx2]}"
                augmented_texts.append(combined_text)
                augmented_labels.append(labels[idx1])
        
        return augmented_texts, augmented_labels

class StyleEmbeddingTrainer:
    """ë¬¸ì²´ ì„ë² ë”© ëª¨ë¸ íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, model_name: str = "klue/bert-base", 
                 output_dir: str = "style_models"):
        self.model_name = model_name
        self.output_dir = output_dir
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = None
        self.data_processor = StyleDataProcessor()
        
        # ê°€ì†í™” ì„¤ì •
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.use_mixed_precision = torch.cuda.is_available()
        self.use_xformers = XFORMERS_AVAILABLE and torch.cuda.is_available()
        self.use_flash_attn = FLASH_ATTN_AVAILABLE and torch.cuda.is_available()
        
        logger.info(f"Device: {self.device}")
        logger.info(f"Mixed Precision: {self.use_mixed_precision}")
        logger.info(f"XFormers: {self.use_xformers}")
        logger.info(f"Flash Attention: {self.use_flash_attn}")
        
        os.makedirs(output_dir, exist_ok=True)
        
    def prepare_data(self, book_text: str, test_size: float = 0.2) -> Dict:
        """ë°ì´í„° ì¤€ë¹„"""
        logger.info("Processing book text...")
        texts, labels = self.data_processor.create_dataset(book_text)
        
        # í•™ìŠµ/ê²€ì¦ ë¶„í• 
        train_texts, val_texts, train_labels, val_labels = train_test_split(
            texts, labels, test_size=test_size, random_state=42, stratify=labels
        )
        
        # ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = StyleDataset(train_texts, train_labels, self.tokenizer)
        val_dataset = StyleDataset(val_texts, val_labels, self.tokenizer)
        
        logger.info(f"Training samples: {len(train_dataset)}")
        logger.info(f"Validation samples: {len(val_dataset)}")
        
        # ë¼ë²¨ ë¶„í¬ í™•ì¸
        unique_labels, counts = np.unique(labels, return_counts=True)
        logger.info("Label distribution:")
        for label, count in zip(unique_labels, counts):
            style_name = [k for k, v in self.data_processor.label_map.items() if v == label][0]
            logger.info(f"  {style_name} ({label}): {count}")
        
        return {
            'train_dataset': train_dataset,
            'val_dataset': val_dataset,
            'train_texts': train_texts,
            'val_texts': val_texts,
            'train_labels': train_labels,
            'val_labels': val_labels
        }
        
    def create_model(self, num_style_classes: int = 8, embedding_dim: int = 384):
        """ëª¨ë¸ ìƒì„±"""
        self.model = StyleEmbeddingModel(
            base_model_name=self.model_name,
            embedding_dim=embedding_dim,
            num_style_classes=num_style_classes,
            use_flash_attn=self.use_flash_attn
        )
        
        if self.use_xformers:
            # XFormers ìµœì í™” ì ìš©
            self.model = torch.compile(self.model, mode="reduce-overhead")
            logger.info("XFormers optimization applied")
        
        self.model.to(self.device)
        return self.model
        
    def train_model(self, data_dict: Dict, num_epochs: int = 3, 
                   batch_size: int = 16, learning_rate: float = 2e-5,
                   progress_callback=None) -> Dict:
        """ëª¨ë¸ í›ˆë ¨"""
        
        if self.model is None:
            self.create_model()
            
        train_dataset = data_dict['train_dataset']
        val_dataset = data_dict['val_dataset']
        
        # ë°ì´í„° ë¡œë”
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True,
            num_workers=4 if torch.cuda.is_available() else 0,
            pin_memory=torch.cuda.is_available()
        )
        
        val_loader = DataLoader(
            val_dataset, 
            batch_size=batch_size * 2,
            shuffle=False,
            num_workers=4 if torch.cuda.is_available() else 0,
            pin_memory=torch.cuda.is_available()
        )
        
        # ì˜µí‹°ë§ˆì´ì €ì™€ ìŠ¤ì¼€ì¤„ëŸ¬
        optimizer = AdamW(self.model.parameters(), lr=learning_rate)
        total_steps = len(train_loader) * num_epochs
        scheduler = get_linear_schedule_with_warmup(
            optimizer, 
            num_warmup_steps=total_steps // 10,
            num_training_steps=total_steps
        )
        
        # Mixed precision ì„¤ì •
        scaler = torch.cuda.amp.GradScaler() if self.use_mixed_precision else None
        
        # í›ˆë ¨ ë£¨í”„
        training_history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}
        
        self.model.train()
        for epoch in range(num_epochs):
            epoch_train_loss = 0
            num_batches = 0
            
            for batch_idx, batch in enumerate(train_loader):
                # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                optimizer.zero_grad()
                
                # Forward pass with mixed precision
                if self.use_mixed_precision:
                    with torch.cuda.amp.autocast():
                        outputs = self.model(input_ids, attention_mask, labels)
                        loss = outputs['loss']
                    
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    outputs = self.model(input_ids, attention_mask, labels)
                    loss = outputs['loss']
                    loss.backward()
                    optimizer.step()
                
                scheduler.step()
                
                epoch_train_loss += loss.item()
                num_batches += 1
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                if progress_callback:
                    progress = ((epoch * len(train_loader) + batch_idx + 1) / 
                              (num_epochs * len(train_loader))) * 100
                    progress_callback(int(progress))
            
            # ê²€ì¦
            val_loss, val_accuracy = self.evaluate_model(val_loader)
            
            avg_train_loss = epoch_train_loss / num_batches
            training_history['train_loss'].append(avg_train_loss)
            training_history['val_loss'].append(val_loss)
            training_history['val_accuracy'].append(val_accuracy)
            
            logger.info(f"Epoch {epoch+1}/{num_epochs}")
            logger.info(f"  Train Loss: {avg_train_loss:.4f}")
            logger.info(f"  Val Loss: {val_loss:.4f}")
            logger.info(f"  Val Accuracy: {val_accuracy:.4f}")
        
        return training_history
    
    def evaluate_model(self, val_loader):
        """ëª¨ë¸ í‰ê°€"""
        self.model.eval()
        total_loss = 0
        correct_predictions = 0
        total_predictions = 0
        
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                if self.use_mixed_precision:
                    with torch.cuda.amp.autocast():
                        outputs = self.model(input_ids, attention_mask, labels)
                else:
                    outputs = self.model(input_ids, attention_mask, labels)
                
                total_loss += outputs['loss'].item()
                
                predictions = torch.argmax(outputs['style_logits'], dim=-1)
                correct_predictions += (predictions == labels).sum().item()
                total_predictions += labels.size(0)
        
        avg_loss = total_loss / len(val_loader)
        accuracy = correct_predictions / total_predictions
        
        self.model.train()
        return avg_loss, accuracy
    
    def save_model(self, model_name: str = None) -> str:
        """ëª¨ë¸ ì €ì¥"""
        if model_name is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_name = f"style_embedding_model_{timestamp}"
        
        model_path = os.path.join(self.output_dir, model_name)
        os.makedirs(model_path, exist_ok=True)
        
        # ëª¨ë¸ ì €ì¥
        torch.save(self.model.state_dict(), os.path.join(model_path, "pytorch_model.bin"))
        
        # í† í¬ë‚˜ì´ì € ì €ì¥
        self.tokenizer.save_pretrained(model_path)
        
        # ì„¤ì • ì €ì¥
        config = {
            'model_name': self.model_name,
            'embedding_dim': self.model.embedding_dim,
            'num_style_classes': len(self.data_processor.label_map),
            'label_map': self.data_processor.label_map,
            'style_patterns': self.data_processor.style_patterns
        }
        
        with open(os.path.join(model_path, "config.json"), 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Model saved to: {model_path}")
        return model_path
    
    def extract_embeddings(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """í…ìŠ¤íŠ¸ì—ì„œ ì„ë² ë”© ì¶”ì¶œ"""
        self.model.eval()
        embeddings = []
        
        with torch.no_grad():
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                
                encoding = self.tokenizer(
                    batch_texts,
                    truncation=True,
                    padding=True,
                    max_length=512,
                    return_tensors='pt'
                )
                
                input_ids = encoding['input_ids'].to(self.device)
                attention_mask = encoding['attention_mask'].to(self.device)
                
                if self.use_mixed_precision:
                    with torch.cuda.amp.autocast():
                        outputs = self.model(input_ids, attention_mask)
                else:
                    outputs = self.model(input_ids, attention_mask)
                
                batch_embeddings = outputs['style_embeddings'].cpu().numpy()
                embeddings.append(batch_embeddings)
        
        return np.vstack(embeddings)

def create_training_gui():
    """ë¬¸ì²´ ì„ë² ë”© ëª¨ë¸ í›ˆë ¨ìš© Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    
    # ì „ì—­ ìƒíƒœ ê´€ë¦¬
    trainer = None
    book_text_content = None
    
    def process_book_file(file_path, min_sentence_length):
        """ì±… íŒŒì¼ ì²˜ë¦¬"""
        nonlocal book_text_content
        
        if file_path is None:
            return None, "íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”."
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                book_text_content = f.read()
            
            # í…ìŠ¤íŠ¸ í†µê³„
            sentences = StyleDataProcessor().extract_sentences(book_text_content)
            sentences = [s for s in sentences if len(s) >= min_sentence_length]
            
            stats = f"""
ğŸ“Š **í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼:**
- ì´ ë¬¸ì ìˆ˜: {len(book_text_content):,}
- ì¶”ì¶œëœ ë¬¸ì¥ ìˆ˜: {len(sentences):,}
- í‰ê·  ë¬¸ì¥ ê¸¸ì´: {np.mean([len(s) for s in sentences]):.1f}ì
- ìµœì†Œ ë¬¸ì¥ ê¸¸ì´: {min_sentence_length}ì ì´ìƒ

âœ… íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ! ì´ì œ ëª¨ë¸ í›ˆë ¨ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            """
            
            return book_text_content, stats
            
        except Exception as e:
            book_text_content = None
            return None, f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    def train_embedding_model(book_text, model_name, num_epochs, batch_size, 
                            learning_rate, embedding_dim, progress=gr.Progress()):
        """ì„ë² ë”© ëª¨ë¸ í›ˆë ¨"""
        nonlocal trainer
        
        if book_text is None or not book_text:
            return "ë¨¼ì € ì±… íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”.", "", ""
        
        try:
            # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
            trainer = StyleEmbeddingTrainer(model_name=model_name)
            
            # ì§„í–‰ë¥  ì½œë°±
            def update_progress(value):
                progress(value / 100, desc=f"ëª¨ë¸ í›ˆë ¨ ì¤‘... {value}%")
            
            # ë°ì´í„° ì¤€ë¹„
            progress(10, desc="ë°ì´í„° ì¤€ë¹„ ì¤‘...")
            data_dict = trainer.prepare_data(book_text)
            
            # ëª¨ë¸ ìƒì„±
            progress(20, desc="ëª¨ë¸ ìƒì„± ì¤‘...")
            trainer.create_model(embedding_dim=embedding_dim)
            
            # í›ˆë ¨ ì‹œì‘
            progress(30, desc="í›ˆë ¨ ì‹œì‘...")
            history = trainer.train_model(
                data_dict=data_dict,
                num_epochs=num_epochs,
                batch_size=batch_size,
                learning_rate=learning_rate,
                progress_callback=update_progress
            )
            
            # ëª¨ë¸ ì €ì¥
            progress(95, desc="ëª¨ë¸ ì €ì¥ ì¤‘...")
            model_path = trainer.save_model()
            
            progress(100, desc="ì™„ë£Œ!")
            
            # í›ˆë ¨ ê²°ê³¼ ìš”ì•½
            final_train_loss = history['train_loss'][-1]
            final_val_loss = history['val_loss'][-1]
            final_val_acc = history['val_accuracy'][-1]
            
            result_summary = f"""
ğŸ‰ **í›ˆë ¨ ì™„ë£Œ!**

ğŸ“ˆ **ìµœì¢… ê²°ê³¼:**
- í›ˆë ¨ ì†ì‹¤: {final_train_loss:.4f}
- ê²€ì¦ ì†ì‹¤: {final_val_loss:.4f}
- ê²€ì¦ ì •í™•ë„: {final_val_acc:.4f}

ğŸ’¾ **ëª¨ë¸ ì €ì¥ ê²½ë¡œ:** {model_path}
"""
            
            # í›ˆë ¨ ê³¼ì • ë°ì´í„°
            history_text = "Epoch\tTrain Loss\tVal Loss\tVal Accuracy\n"
            for i, (tl, vl, va) in enumerate(zip(history['train_loss'], 
                                               history['val_loss'], 
                                               history['val_accuracy'])):
                history_text += f"{i+1}\t{tl:.4f}\t{vl:.4f}\t{va:.4f}\n"
            
            return result_summary, history_text, model_path
            
        except Exception as e:
            return f"í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "", ""
    
    def test_embeddings(model_path, test_text):
        """ì„ë² ë”© í…ŒìŠ¤íŠ¸"""
        nonlocal trainer
        
        if trainer is None or not model_path:
            return "ë¨¼ì € ëª¨ë¸ì„ í›ˆë ¨í•´ì£¼ì„¸ìš”."
        
        if not test_text or not test_text.strip():
            return "í…ŒìŠ¤íŠ¸í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
        
        try:
            # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ì„ë² ë”© ì¶”ì¶œ
            embeddings = trainer.extract_embeddings([test_text])
            
            # ìŠ¤íƒ€ì¼ ì˜ˆì¸¡
            trainer.model.eval()
            with torch.no_grad():
                encoding = trainer.tokenizer(
                    test_text,
                    truncation=True,
                    padding=True,
                    max_length=512,
                    return_tensors='pt'
                )
                
                input_ids = encoding['input_ids'].to(trainer.device)
                attention_mask = encoding['attention_mask'].to(trainer.device)
                
                outputs = trainer.model(input_ids, attention_mask)
                style_probs = F.softmax(outputs['style_logits'], dim=-1)
                style_probs = style_probs.cpu().numpy()[0]
            
            # ê²°ê³¼ í¬ë§·íŒ…
            label_map = trainer.data_processor.label_map
            reverse_label_map = {v: k for k, v in label_map.items()}
            
            result = "ğŸ¯ **ìŠ¤íƒ€ì¼ ë¶„ì„ ê²°ê³¼:**\n\n"
            for i, prob in enumerate(style_probs):
                style_name = reverse_label_map[i]
                result += f"- {style_name}: {prob:.3f} ({prob*100:.1f}%)\n"
            
            result += f"\nğŸ“Š **ì„ë² ë”© ì°¨ì›:** {embeddings.shape[1]}"
            
            return result
            
        except Exception as e:
            return f"í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}"

    # Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬ì„±
    with gr.Blocks(title="Style Embedding Model Trainer") as demo:
        gr.Markdown("# ğŸ¨ ë¬¸ì²´ ì„ë² ë”© ëª¨ë¸ íŠ¸ë ˆì´ë„ˆ")
        gr.Markdown("ì±… í•œ ê¶Œì„ ì—…ë¡œë“œí•˜ì—¬ ë¬¸ì²´ ì„ë² ë”© ëª¨ë¸ì„ í›ˆë ¨í•˜ì„¸ìš”!")
        
        with gr.Tabs():
            with gr.TabItem("ğŸ“š ë°ì´í„° ì¤€ë¹„"):
                gr.Markdown("### 1. ì±… íŒŒì¼ ì—…ë¡œë“œ")
                book_file = gr.File(
                    label="í…ìŠ¤íŠ¸ íŒŒì¼ (.txt)",
                    file_types=[".txt"]
                )
                
                min_sentence_length = gr.Slider(
                    minimum=10,
                    maximum=100,
                    value=20,
                    step=5,
                    label="ìµœì†Œ ë¬¸ì¥ ê¸¸ì´ (ë¬¸ì)"
                )
                
                process_btn = gr.Button("ğŸ“Š íŒŒì¼ ë¶„ì„", variant="primary")
                
                book_stats = gr.Markdown()
                book_text_state = gr.State()
                
            with gr.TabItem("ğŸ‹ï¸ ëª¨ë¸ í›ˆë ¨"):
                gr.Markdown("### í›ˆë ¨ ì„¤ì •")
                
                with gr.Row():
                    with gr.Column():
                        model_name = gr.Dropdown(
                            choices=[
                                "klue/bert-base",
                                "klue/roberta-base", 
                                "beomi/KcELECTRA-base",
                                "monologg/kobert"
                            ],
                            value="klue/bert-base",
                            label="ë² ì´ìŠ¤ ëª¨ë¸"
                        )
                        
                        num_epochs = gr.Slider(
                            minimum=1,
                            maximum=10,
                            value=3,
                            step=1,
                            label="ì—í¬í¬ ìˆ˜"
                        )
                        
                        batch_size = gr.Slider(
                            minimum=4,
                            maximum=64,
                            value=16,
                            step=4,
                            label="ë°°ì¹˜ í¬ê¸°"
                        )
                    
                    with gr.Column():
                        learning_rate = gr.Number(
                            value=2e-5,
                            label="í•™ìŠµë¥ "
                        )
                        
                        embedding_dim = gr.Slider(
                            minimum=128,
                            maximum=768,
                            value=384,
                            step=64,
                            label="ì„ë² ë”© ì°¨ì›"
                        )
                
                train_btn = gr.Button("ğŸš€ í›ˆë ¨ ì‹œì‘", variant="primary", size="lg")
                
                training_result = gr.Markdown()
                training_history = gr.Textbox(
                    label="í›ˆë ¨ ê³¼ì •",
                    lines=10,
                    interactive=False
                )
                model_path_state = gr.State()
                
            with gr.TabItem("ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸"):
                gr.Markdown("### ì„ë² ë”© í…ŒìŠ¤íŠ¸")
                
                test_text = gr.Textbox(
                    label="í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸",
                    placeholder="ë¬¸ì²´ë¥¼ ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
                    lines=3,
                    value="ê·¸ëŠ” ì¡°ìš©íˆ ë¬¸ì„ ì—´ê³  ë°© ì•ˆìœ¼ë¡œ ë“¤ì–´ê°”ë‹¤."
                )
                
                test_btn = gr.Button("ğŸ” ë¶„ì„í•˜ê¸°", variant="secondary")
                
                test_result = gr.Markdown()
        
        # âš ï¸ í•µì‹¬: ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ë“¤ - ì´ ë¶€ë¶„ì´ ì—†ìœ¼ë©´ ë²„íŠ¼ì´ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!
        process_btn.click(
            fn=process_book_file,
            inputs=[book_file, min_sentence_length],
            outputs=[book_text_state, book_stats]
        )
        
        train_btn.click(
            fn=train_embedding_model,
            inputs=[
                book_text_state, model_name, num_epochs, batch_size,
                learning_rate, embedding_dim
            ],
            outputs=[training_result, training_history, model_path_state],
            show_progress=True
        )
        
        test_btn.click(
            fn=test_embeddings,
            inputs=[model_path_state, test_text],
            outputs=test_result
        )
        
        # ê°€ì†í™” ì •ë³´
        gr.Markdown("## âš¡ ê°€ì†í™” ìƒíƒœ")
        acceleration_info = f"""
        - **CUDA:** {'âœ…' if torch.cuda.is_available() else 'âŒ'}
        - **XFormers:** {'âœ…' if XFORMERS_AVAILABLE else 'âŒ'}
        - **Flash Attention 2:** {'âœ…' if FLASH_ATTN_AVAILABLE else 'âŒ'}
        """
        gr.Markdown(acceleration_info)
        
        # ì‚¬ìš© ê°€ì´ë“œ
        gr.Markdown("""
        ## ğŸ“– ì‚¬ìš© ë°©ë²•
        
        1. **ğŸ“š ë°ì´í„° ì¤€ë¹„** íƒ­ì—ì„œ `.txt` íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  'ğŸ“Š íŒŒì¼ ë¶„ì„' ë²„íŠ¼ì„ í´ë¦­
        2. **ğŸ‹ï¸ ëª¨ë¸ í›ˆë ¨** íƒ­ì—ì„œ ì„¤ì •ì„ ì¡°ì •í•˜ê³  'ğŸš€ í›ˆë ¨ ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­  
        3. **ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸** íƒ­ì—ì„œ í›ˆë ¨ëœ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸
        
        ### ğŸ’¡ íŒ
        - ìµœì†Œ 10,000ì ì´ìƒì˜ í…ìŠ¤íŠ¸ ê¶Œì¥
        - GPUê°€ ìˆìœ¼ë©´ ë°°ì¹˜ í¬ê¸°ë¥¼ 16-32ë¡œ ì„¤ì •
        - CPUë§Œ ìˆìœ¼ë©´ ë°°ì¹˜ í¬ê¸°ë¥¼ 4-8ë¡œ ì„¤ì •
        
        ### ğŸš¨ ë¬¸ì œ í•´ê²°
        - ë²„íŠ¼ì„ ëˆŒëŸ¬ë„ ë°˜ì‘ì´ ì—†ë‹¤ë©´ ì½˜ì†”(F12)ì—ì„œ ì—ëŸ¬ë¥¼ í™•ì¸í•˜ì„¸ìš”
        - íŒŒì¼ ì—…ë¡œë“œ í›„ ë°˜ë“œì‹œ 'ğŸ“Š íŒŒì¼ ë¶„ì„' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”
        - ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì—¬ì£¼ì„¸ìš”
        """)
    
    return demo

if __name__ == "__main__":
    print("ğŸ” ê°€ì†í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸:")
    print(f"  - PyTorch CUDA: {'âœ…' if torch.cuda.is_available() else 'âŒ'}")
    print(f"  - XFormers: {'âœ…' if XFORMERS_AVAILABLE else 'âŒ'}")
    print(f"  - Triton: {'âœ…' if TRITON_AVAILABLE else 'âŒ'}")
    print(f"  - Flash Attention 2: {'âœ…' if FLASH_ATTN_AVAILABLE else 'âŒ'}")
    
    if torch.cuda.is_available():
        print(f"  - GPU: {torch.cuda.get_device_name()}")
        print(f"  - VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    
    print("ğŸš€ Gradio ì¸í„°í˜ì´ìŠ¤ ì‹œì‘ ì¤‘...")
    
    # ì‹¤ì œ ì‹¤í–‰ ì½”ë“œ
    demo = create_training_gui()
    demo.launch(
        server_name="127.0.0.1",  # localhostë¡œ ë³€ê²½
        server_port=7860,
        share=False,
        debug=True,
        show_error=True
    )
    
    print("âœ… ë¬¸ì²´ ì„ë² ë”© íŠ¸ë ˆì´ë„ˆê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("ğŸŒ ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:7860 ë¡œ ì ‘ì†í•˜ì„¸ìš”!")